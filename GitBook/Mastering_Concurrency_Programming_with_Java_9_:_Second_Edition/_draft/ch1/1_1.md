# 1 1 Basic concurrency concepts

우선, 동시성의 기본 개념을 소개합니다. 이 책의 나머지 부분을 이해하려면 이러한 개념을 이해해야 합니다.
First of all, let's present the basic concepts of concurrency. You must understand these concepts to follow the rest of the book.

## 동시성 대 병렬성 - Concurrency versus parallelism

동시성과 병렬성은 매우 유사한 개념입니다. 다른 저자는 이러한 개념에 대해 다른 정의를 내리고 있습니다. 가장 많이 받아들여지는 정의는 동시성이 단일 코어를 가진 단일 프로세서에서 둘 이상의 작업을 수행하는 경우라고 말합니다. 이 경우 운영체제의 작업 스케줄러가 한 작업에서 다른 작업으로 신속하게 전환되므로 모든 작업이 동시에 실행되는 것으로 보입니다. 동일한 정의는 병렬 처리가 여러 컴퓨터, 프로세서 또는 프로세서 내부의 코어에서 동시에 실행되는 둘 이상의 작업을 가지고 있는 것으로 말합니다.
Concurrency and parallelism are very similar concepts. Different authors give different definitions for these concepts. The most accepted definition talks about concurrency as being when you have more than one task in a single processor with a single core. In this case, the operating system's task scheduler quickly switches from one task to another, so it seems that all the tasks run simultaneously. The same definition talks about parallelism as being when you have more than one task running simultaneously on different computers, processors, or cores inside a processor.

또 다른 정의는 시스템에서 동시에 실행되는 둘 이상의 태스크 (다른 태스크)가 있을 때 동시성에 대해 이야기합니다. 또 다른 정의는 데이터 집합의 다른 부분에서 동시에 실행되는 동일한 태스크의 다른 인스턴스가 있을 때의 병렬 처리를 설명합니다.
Another definition talks about concurrency being when you have more than one task (different tasks) that run simultaneously on your system. Yet another definition discusses parallelism as being when you have different instances of the same task that run simultaneously over different parts of a dataset.

마지막 정의는 병렬 처리가 시스템에서 동시에 실행되는 둘 이상의 태스크를 갖고 있고 프로그래머가 태스크 및 공유 리소스에 대한 액세스와 동기화 해야하는 다양한 기술과 메커니즘을 설명하는 방법으로 동시성에 대해 이야기 할 때 이야기합니다.
The last definition talks about parallelism being when you have more than one task that runs simultaneously in your system and talks about concurrency as a way to explain the different techniques and mechanisms the programmer has to synchronize with the tasks and their access to shared resources.

보시다시피 두 개념 모두 매우 유사하며 이러한 유사성은 멀티 코어 프로세서의 개발과 함께 증가했습니다.
As you can see, both concepts are very similar, and this similarity has increased with the development of multicore processors.

## 동기화 - Synchronization

동시성에서 우리는 원하는 결과를 얻기 위해 두 개 이상의 작업을 조정하는 것으로 **동기화**를 정의 할 수 있습니다. 동기화에는 두 가지 종류가 있습니다.
In concurrency, we can define **synchronization** as the coordination of two or more tasks to get the desired results. We have two kinds of synchronization:

* **제어 동기화** : 예를 들어 한 태스크가 다른 태스크의 끝 부분에 의존하는 경우 두 번째 태스크는 첫 번째 태스크가 완료되기 전에 시작할 수 없습니다
* **Control synchronization** : When, for example, one task depends on the end of another task, the second task can't start before the first has finished
* **데이터 액세스 동기화** : 두 개 이상의 작업이 공유 변수에 액세스 할 수 있고 하나의 작업만 변수에 액세스 할 수 있는 경우
* **Data access synchronization** : When two or more tasks have access to a shared variable and only one of the tasks can access the variable

동기화와 밀접하게 관련된 개념은 **임계 구역**입니다. 임계 구역은 공유 리소스에 대한 액세스 권한으로 인해 한 번에 하나의 작업으로만 실행할 수 있는 코드 조각입니다. **상호 배제**는 이 요구 사항을 보장하는 데 사용되는 메커니즘이며 다른 방식으로 구현 될 수 있습니다.
A concept closely related to synchronization is **critical section**. A critical section is a piece of code that can be only executed by one task at a time because of its access to a shared resource. **Mutual exclusion** is the mechanism used to guarantee this requirement and can be implemented in different ways.

동기화를 사용하면 동시 작업에서 발생할 수 있는 오류 (이 장의 뒷부분에서 설명 함)를 피할 수 있지만 알고리즘에 약간의 오버 헤드가 발생합니다. 작업의 수를 매우 신중하게 계산해야 합니다. 병렬 알고리즘에서 상호 작용없이 독립적으로 수행 할 수 있습니다. 동시 알고리즘의 세분성입니다. 굵은 입도 (상호 통신이 적은 큰 작업) 인 경우 동기화로 인한 오버 헤드가 낮습니다. 그러나 어쩌면 시스템의 모든 코어에서 이익을 얻지 못할 수도 있습니다. 세분화 된 세분성 (상호 통신이 많은 작은 작업)을 사용하면 동기화로 인한 오버 헤드가 높아지고 알고리즘 처리량이 좋지 않을 수 있습니다.
Keep in mind that synchronization helps you avoid some errors you might have with concurrent tasks (they will be described later in this chapter), but it introduces some overhead to your algorithm. You have to calculate the number of tasks very carefully, which can be performed independently without intercommunication you will have in your parallel algorithm. It's the **granularity** of your concurrent algorithm. If you have a **coarse-grained granularity** (big tasks with low intercommunication), the overhead due to synchronization will be low. However, maybe you won't benefit from all the cores of your system. If you have a **fine-grained granularity** (small tasks with high intercommunication), the overhead due to synchronization will be high, and perhaps the throughput of your algorithm won't be good.

동시 시스템에서 동기화를 얻는 다른 메커니즘이 있습니다. 이론적 관점에서 가장 많이 사용되는 메커니즘은 다음과 같습니다.
There are different mechanisms to get synchronization in a concurrent system. The most popular mechanisms from a theoretical point of view are

* **세마포어** (Semaphore) : 세마포어 (semaphore)는 하나 이상의 자원 단위에 대한 액세스를 제어하는 데 사용할 수있는 메커니즘입니다. 변수의 값을 관리하기 위해 사용할 수있는 자원의 수와 원자 연산 두 개를 저장하는 변수가 있습니다. **뮤텍스** (**상호 배제**의 약자)는 세 가지 종류의 세마포어로, 리소스가 사용 가능하고 리소스가 사용 중이라는 두 가지 값만 사용할 수 있으며, 뮤텍스를 사용 중으로 설정하는 프로세스 만이이를 해제 할 수 있습니다. 뮤텍스는 중요한 섹션을 보호하여 경쟁 조건을 피하는 데 도움이 될 수 있습니다.
* **Semaphore**: A semaphore is a mechanism that can be used to control the access to one or more units of a resource. It has a variable that stores the number of resources that can be used and two atomic operations to manage the value of the variable. A **mutex** (short for **mutual exclusion**) is a special kind of semaphore that can take only two values (resource is free and resource is busy), and only the process that sets the mutex to busy can release it. A mutex can help you to avoid race conditions by protecting a critical section.
* **모니터** : 모니터는 공유 자원에 대해 상호 배타를 가져 오는 메커니즘입니다. mutex, 조건 변수 및 조건을 기다리고 조건을 알리는 두 가지 작업이 있습니다. 조건 신호를 보내면 대기중인 작업 중 하나만 실행을 계속합니다.
* **Monitor**: A monitor is a mechanism to get mutual exclusion over a shared resource. It has a mutex, a condition variable, and two operations to wait for the condition and signal the condition. Once you signal the condition, only one of the tasks that are waiting for it continues with its execution.

이 장에서 배우게 될 동기화와 관련된 마지막 개념은 스레드 안전입니다. 공유 데이터의 모든 사용자가 동기화 메커니즘으로 보호되는 경우 코드 (또는 메소드 또는 객체)가 스레드로부터 안전합니다. 데이터의 non-blocking, compare-and-swap (CAS) 프리미티브는 변경 불가능하므로 병행 응용 프로그램에서 문제없이 코드를 사용할 수 있습니다.
The last concept related to synchronization you're going to learn in this chapter is **thread safety**. A piece of code (or a method or an object) is **thread-safe** if all the users of shared data are protected by synchronization mechanisms. A non-blocking, **compare-and-swap** (**CAS**) primitive of the data is immutable, so you can use that code in a concurrent application without any problems.

## 변경 불가능한 객체 - Immutable object

불변 개체는 매우 특별한 특성을 가진 개체입니다. 초기화 후에는 가시 상태 (특성 값)를 수정할 수 없습니다. 변경할 수 없는 개체를 수정하려면 새 개체를 만들어야합니다.
An **immutable object** is an object with a very special characteristic. You can't modify its visible state (the value of its attributes) after its initialization. If you want to modify an immutable object, you have to create a new one.

주요 장점은 스레드로부터 안전하다는 것입니다. 동시 응용 프로그램에서 아무 문제없이 사용할 수 있습니다.
Its main advantage is that it is thread-safe. You can use it in concurrent applications without any problem.

불변 객체의 예는 Java의 String 클래스입니다. String 객체에 새 값을 할당하면 새 값을 만듭니다.
An example of an immutable object is the String class in Java. When you assign a new value to a String object, you are creating a new one.

## 원자 연산 및 변수 - Atomic operations and variables

원자적 연산은 프로그램의 나머지 작업에 순간적으로 발생하는 것처럼 보이는 일종의 연산입니다. 동시 응용 프로그램에서는 동기화 메커니즘을 사용하여 전체 작업에 대한 중요한 섹션이있는 원자 작업을 구현할 수 있습니다.
An **atomic operation** is a kind of operation that appears to occur instantaneously to the rest of the tasks of the program. In a concurrent application, you can implement an atomic operation with a critical section to the whole operation using a synchronization mechanism.

원자 변수는 원자 연산을 통해 값을 설정하고 가져 오는 일종의 변수입니다. 동기화 메커니즘을 사용하여 원자 변수를 구현하거나 동기화가 필요없는 CAS를 사용하여 잠금없는 방식으로 원자 변수를 구현할 수 있습니다.
An **atomic variable** is a kind of variable that has atomic operations to set and get its value. You can implement an atomic variable using a synchronization mechanism or in a lock-free manner using CAS that doesn't need synchronization.

## 공유 메모리 대 메시지 전달 - Shared memory versus message passing

작업은 서로 다른 두 가지 방법을 사용하여 서로 통신 할 수 있습니다. 첫 번째는 공유 메모리이며 일반적으로 작업이 동일한 컴퓨터에서 실행될 때 사용됩니다. 작업은 값을 쓰고 읽는 동일한 메모리 영역을 사용합니다. 문제를 피하려면이 공유 메모리에 대한 액세스가 동기화 메커니즘으로 보호되는 중요한 섹션에 있어야합니다.
Tasks can use two different methods to communicate with each other. The first one is **shared memory** and, normally, it is used when the tasks are running on the same computer. The tasks use the same memory area where they write and read values. To avoid problems, the access to this shared memory has to be in a critical section protected by a synchronization mechanism.

다른 동기화 메커니즘은 메시지 전달이며 일반적으로 작업이 다른 컴퓨터에서 실행될 때 사용됩니다. 작업이 다른 작업과 통신해야하는 경우 미리 정의 된 프로토콜을 따르는 메시지를 보냅니다. 이 통신은 보낸 사람이 메시지를 보낸 후 보낸 사람이 계속 실행하면 응답을 기다리는 것이 차단되거나 계속 비동기로 유지되는 경우 동기식이 될 수 있습니다.
The other synchronization mechanism is **message passing** and, normally, it is used when the tasks are running on different computers. When tasks needs to communicate with another, it sends a message that follows a predefined protocol. This communication can be synchronous if the sender keeps it blocked waiting for a response or asynchronous if the sender continues with their execution after sending the message.